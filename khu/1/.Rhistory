# 1. 주요변수 탐색
ames %>%
ggplot(aes(x = Sale_Price)) +
geom_histogram(bins = 50, col = "orange",
fill = "yellow")
# 1. 주요변수 탐색
ames %>%
ggplot(aes(x = Sale_Price)) +
geom_histogram(bins = 30,
col = "orange", fill = "yellow") # 슥삭 꾸며주기
# 1. 주요변수 탐색
ames %>%
ggplot(aes(x = Sale_Price)) +
geom_histogram(bins = 20,
col = "orange", fill = "yellow") # 슥삭 꾸며주기
# 1. 주요변수 탐색
ames %>%
ggplot(aes(x = Sale_Price)) +
geom_histogram(bins = 100,
col = "orange", fill = "yellow") # 슥삭 꾸며주기
# 1. 주요변수 탐색
ames %>%
ggplot(aes(x = Sale_Price)) +
geom_histogram(bins = 10,
col = "orange", fill = "yellow") # 슥삭 꾸며주기
ames %>%
ggplot(aes(x = Sale_Price)) +
geom_histogram(bins = 30) +
scal_x_log10()
ames %>%
ggplot(aes(x = Sale_Price)) +
geom_histogram(bins = 30) +
scal_x_log10()
ames %>%
ggplot(aes(x = Sale_Price)) +
geom_histogram(bins = 50) +
scal_x_log10()
ames %>%
ggplot(aes(x = Sale_Price)) +
geom_histogram(bins = 30) +
scale_x_log10()
ames %<>%
mutate(Sale_Price = log10(Sale_Price))
# 1. 주요변수 탐색
ames %>%
ggplot(aes(x = Sale_Price)) + # Sale_Price를 살펴보자
geom_histogram(bins = 30, # 히스토그램의 간격 조정
col = "orange", fill = "yellow") # 슥삭 꾸며주기
ames_split <- initial_split(ames, prop = 0.80)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)
dim(ames_train)
ames_split
# 2. 데이터 전처리
# 2.1. 데이터 분할
set.seed(123)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split) # Analysis를 학습으로
ames_test  <-  testing(ames_split) # Assess를 테스트로
dim(ames_train)
ames_split <- rsample::initial_split(ames, prop = 0.80, strata = Sale_Price) # 집값 기준으로 데이터 분할해주세욧
ames_train <- training(ames_split) # Analysis를 학습으로 적재
ames_test  <-  testing(ames_split) # Assess를 테스트로 적재
dim(ames_train)
pacman::p_load(tidyverse, magrittr, # helper packages
tidymodels,caret, # modeling packages
kknn)# engine package
read_csv("Heart.csv") -> heart
data %>% as_tibble(data)
# 1. Data Background
read_csv("Heart.csv") -> heart
heart
# 1. Data Background
read.csv("Heart.csv") -> heart
heart
# 1. Data Background
read_csv("Heart.csv") -> heart
heart
library(tidyverse)
detach(tidyverse)
detach.("tidyverse")
detach("tidyverse")
detach(package = tidyverse)
detach("package:dplyr", unload = TRUE)
library(dplyr)
pacman::p_load(tidyverse, magrittr, # helper packages
tidymodels, # modeling packages
gain, caret, e1071, pROC) # 일단 설치하래서 하는 패키지
# 2. 데이터 살펴보기
readr::read_csv("Heart.csv") -> heart
heart %>% glimpse()
heart %<>%
select(-X1)
head(heart)
heart %<>%
na.omit() # 결측값을 제거해라
heart %<>%
mutate(ChestPain = as.factor(ChestPain),
Thal = as.factor(Thal),
AHD = as.factor(AHD))
heart %>%  skimr::skim()
heart %>%  skimr::skim() # 개좋네 ㅋㅋ
heart %>%  skimr::skim() # 개좋네 ㅋㅋ
heart %>%
group_by(AHD) %>%
skimr::skim()
heart %>%
group_by(AHD) %>%
skimr::skim()
set.seed(123)
heart_split <- rsample::initial_split(heart,
prop = 0.8,
strata = AHD)
heart_train <- heart_split %>% training()
heart_test <- heart_split %>% testing()
bayes_model <-
naive_Bayes() %>%  # 사용 모델
set_engine("klaR")
library(discrim) #  tidymodel에서 나이브베이즈 옵션 주는 패키지
pacman::p_load(tidyverse, magrittr, # helper packages
tidymodels, discrim, # modeling packages
gain, caret, e1071, pROC) # 일단 설치하래서 하는 패키지
args(naive_Bayes)
bayes_model <-
naive_Bayes() %>%  # 사용 모델
set_engine("klaR")
bayes_recipe <-
recipe(AHD ~ ., data = heart_train)
summary(bayes_recipe)
bayes_workflow <-
workflow() %>%
add_model(bayes_model) %>%
add_recipe(bayes_recipe)
bayes_workflow
bayes_train_fit <-
bayes_workflow %>%
fit(data = heart_train)
pacman::p_load(tidyverse, magrittr, # helper packages
tidymodels, discrim, klaR, # modeling packages
gain, caret, e1071, pROC) # 일단 설치하래서 하는 패키지
bayes_train_fit <-
bayes_workflow %>%
fit(data = heart_train)
# 모델 훈련 결과 확인
bayes_train_fit %>%
pull_workflow_fit()
bayes_train_pred <-
predict(bayes_train_fit,
heart_train,
type = "prob") %>%
bind_cols(predict(bayes_train_fit,
heart_train)) %>%
bind_cols(heart_train %>%
select(AHD)) %>%
print()
bayes_train_pred <-
predict(bayes_train_fit,
heart_train,
type = "prob") %>%
bind_cols(predict(bayes_train_fit,
heart_train)) %>%
bind_cols(heart_train %>%
select(AHD)) %>%
print()
predict(bayes_train_fit,
heart_train,
type = "prob")
predict(bayes_train_fit,
heart_train,
type = "prob") %>%
bind_cols(predict(bayes_train_fit,
heart_train))
heart_train %>% select(AHD)
heart_train
heart_train %>% select(AHD)
heart_train %>% select(AHD)
heart_train %>%
select(AHD)
heart_train
heart_train %>%
select(AHD)
heart_train %>%
dplyr::select(AHD)
bayes_train_pred <-
predict(bayes_train_fit,
heart_train,
type = "prob") %>%
bind_cols(predict(bayes_train_fit,
heart_train)) %>%
bind_cols(heart_train %>% dpldyr::elect(AHD)) %>%
print()
bayes_train_pred <-
predict(bayes_train_fit,
heart_train,
type = "prob") %>%
bind_cols(predict(bayes_train_fit,
heart_train)) %>%
bind_cols(heart_train %>% dplyr::elect(AHD)) %>%
print()
bayes_train_pred <-
predict(bayes_train_fit,
heart_train,
type = "prob") %>%
bind_cols(predict(bayes_train_fit,
heart_train)) %>%
bind_cols(heart_train %>% dplyr::select(AHD)) %>%
print()
bayes_train_conf <-
bayes_train_pred  %>%
conf_mat(truth = AHD,
estimate = .pred_class)
bayes_train_conf
autoplot(bayes_train_conf, type = "heatmap") # mosaic 여기는 안해도 된다
autoplot(bayes_train_conf, type = "mosaic")
summary(bayes_train_conf)
bayes_train_pred %>%
roc_auc(truth = AHD,
.pred_No)
train_auc <-
bayes_train_pred %>%
roc_curve(truth = AHD,
estimate = .pred_No) %>%
mutate(model = "train_auc")
autoplot(train_auc)
# gain 커브
bayes_train_pred %>%
gain_curve(truth = AHD,
estimate = .pred_No) %>%
autoplot()
# lift 커브
bayes_train_pred %>%
lift_curve(truth = AHD,
estimate = .pred_No) %>%
autoplot()
pacman::p_load(tidyverse, magrittr, # helper packages
tidymodels, discrim, klaR, # modeling packages
caret, e1071, pROC) # 일단 설치하래서 하는 패키지
read_csv("wine.csv") -> wine
wine %>%
skimr::skim()
wine %<>%
mutate(Class = as.factor(Class))
source("~/R/kjsang/khu/1/7.7.ML_DecisionTree.R", echo=TRUE)
rand_forest() %>%
set_engine("ranger") %>%
set_mode("classification") %>%
translate()
pacman::p_load(tidyverse, magrittr, # helper packages
tidymodels, kernlab) # modeling packages
wine %>% glimpse()
# 4. model 만들기
svm_rbf(cost = tune(),
rbf_sigma = tune()) %>%
set_engine("kernlab") %>%
set_mode("classification") %>%
translate()
# 4. model 만들기
svm_rbf(cost = tune(),
rbf_sigma = tune()) %>%
set_engine("kernlab") %>%
set_mode("classification") %>%
translate() -> svm_mod
rec <-
recip(Class ~ ., data = wine) %>%
step_zv(all_predictors()) %>%
step_lincomb(all_numeric())
rec <-
recipe(Class ~ ., data = wine) %>%
step_zv(all_predictors()) %>%
step_lincomb(all_numeric())
# 4.2. 부트스트랩
rs <- bootstrap(wine, times = 10)
# 4.2. 부트스트랩
rs <- bootstraps(wine, times = 10)
rs
roc_vals <- metric_set(roc_auc)
roc_vals
ctrl <- control_grid(verbose = F,
save_pred = T)
ctrl
# 4.2. 초기모델
# 모델을 그리드에 넣고 레시피를 만듭니다.
set.seed(123)
roc_vals <- metric_set(roc_auc)
roc_vals
formula_res <-
tune_grid(
Class ~ .,
resamples = rs,
metrics = roc_vals,
control = ctrl
)
formula_res <-
svm_mod %>%
tune_grid(Class ~ .,
resamples = rs,
metrics = roc_vals,
control = ctrl)
formula_res
formula_res
estimates <- collect_metrics(formula_res)
show_best(formula_res, metric = "roc_auc")
# 4.3. 레시피로 실행
set.seed(123)
recipe_res <-
svm_mod %>%
tune_grid(rec,
resamples = rs,
metrics = roc_vals,
control = ctrl)
recipe_res
show_best(recipe_res, metric = "roc_auc")
collect_predictions(recipe_res)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_No, color = Class)) +
geom_point(show.legend = FALSE) +
facet_wrap( ~ Class)
recipe_res
show_best(recipe_res, metric = "roc_auc")
collect_predictions(recipe_res)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_No, color = Class)) +
geom_point(show.legend = FALSE) +
facet_wrap( ~ Class)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_Yes, color = Class)) +
geom_point(show.legend = FALSE) +
facet_wrap( ~ Class)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_1, color = Class)) +
geom_point(show.legend = FALSE) +
facet_wrap( ~ Class)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_2, color = Class)) +
geom_point(show.legend = FALSE) +
facet_wrap( ~ Class)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_3, color = Class)) +
geom_point(show.legend = FALSE) +
facet_wrap( ~ Class)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_2, color = Class)) +
geom_point(show.legend = FALSE) +
facet_wrap( ~ Class)
set.seed(123)
formula_res <-
svm_mod %>%
tune_grid(Class ~ .,
resamples = rs,
metrics = roc_vals,
control = ctrl)
formula_res
estimates <- collect_metrics(formula_res)
show_best(formula_res, metric = "roc_auc")
# 4.3. 레시피로 실행
set.seed(123)
recipe_res <-
svm_mod %>%
tune_grid(rec,
resamples = rs,
metrics = roc_vals,
control = ctrl)
recipe_res
show_best(recipe_res, metric = "roc_auc")
collect_predictions(recipe_res)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_2, color = Class)) +
geom_point(show.legend = FALSE) +
facet_wrap( ~ Class)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_2)) +
geom_point(show.legend = FALSE)
# 2. 데이터 살펴보기
wine %>% skimr::skim()
wine %>% glimpse()
wine
# 3. 데이터 전처리
wine %<>%
mutate(Class = as.factor(Class))
# 4.1. 모델만들기 기초
svm_rbf(cost = tune(),
rbf_sigma = tune()) %>%
set_engine("kernlab") %>%
set_mode("classification") %>%
translate() -> svm_mod
rec <-
recipe(Class ~ ., data = wine) %>%
step_zv(all_predictors()) %>%
step_lincomb(all_numeric())
# 4.2. 부트스트랩
rs <- bootstraps(wine, times = 10)
rs
roc_vals <- metric_set(roc_auc)
roc_vals
ctrl <- control_grid(verbose = F,
save_pred = T)
ctrl
# 4.2. 초기모델
# 모델을 그리드에 넣고 레시피를 만듭니다.
set.seed(123)
formula_res <-
svm_mod %>%
tune_grid(Class ~ .,
resamples = rs,
metrics = roc_vals,
control = ctrl)
formula_res
estimates <- collect_metrics(formula_res)
show_best(formula_res, metric = "roc_auc")
# 4.3. 레시피로 실행
set.seed(123)
recipe_res <-
svm_mod %>%
tune_grid(rec,
resamples = rs,
metrics = roc_vals,
control = ctrl)
recipe_res
show_best(recipe_res, metric = "roc_auc")
collect_predictions(recipe_res)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_2)) +
geom_point(show.legend = FALSE)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_2))
collect_predictions(recipe_res)
recipe_res
recipe_res
show_best(recipe_res, metric = "roc_auc")
collect_predictions(recipe_res)
augment(recipe_res) %>%
ggplot(aes(Clsss, .pred_2)) +
geom_point(show.legend = FALSE) # 이거 왜 에러나지?
augment(recipe_res) %>%
ggplot(aes(x = Clsss, .pred_2)) +
geom_point(show.legend = FALSE) # 이거 왜 에러나지?
recipe_res
wine %>% glimpse()
augment(recipe_res) %>%
ggplot(aes(Alcohol, .pred_2)) +
geom_point(show.legend = FALSE) # 이거 왜 에러나지?
augment(recipe_res) %>%
ggplot(aes(Alcohol, .pred_2, color = Class)) +
geom_point(show.legend = FALSE) # 이거 왜 에러나지?
augment(recipe_res) %>%
ggplot(aes(Alcohol, .pred_2, color = Class)) +
geom_point(show.legend = FALSE) +
facet_wrap(~Class)
tidy(recipe_res)
svm_mod %>%
fit(Class ~ .,
data = wine) -> svm_fit
# 3.1. 데이터 분할
set.seed(123)
wine_split <- rsample::initial_split(raw_wine, prop = 0.80, strata = Class) # 클래스 값 기준으로 데이터 분할해주세욧
wine_train <- training(wine_split) # Analysis를 학습으로 적재
wine_test  <-  testing(wine_split) # Assess를 테스트로 적재
wine_train %>%  glimpse()
# 3.1. 데이터 분할
set.seed(123)
wine_split <- rsample::initial_split(wine, prop = 0.80, strata = Class) # 클래스 값 기준으로 데이터 분할해주세욧
wine_train <- training(wine_split) # Analysis를 학습으로 적재
wine_test  <-  testing(wine_split) # Assess를 테스트로 적재
wine_train %>%  glimpse()
svm_mod %>%
fit(Class ~ .,
data = wine_train) -> svm_fit
# 4.1. 모델만들기 기초
svm_rbf() %>%
set_engine("kernlab") %>%
set_mode("classification") %>%
translate() -> svm_mod
svm_mod %>%
fit(Class ~ .,
data = wine_train) -> svm_fit
svm_fit
tune <- caret::trainControl(
method = "repeatedcv", # cross-validation 반복
number = 10, # 훈련데이터의 fold 수
repeats = 5 # cv 반복횟수
)
model1 <- caret::train(Class ~ ., data = wine_train,
method = "kernlab",
trControl = tune)
# 4.1. 모델만들기 기초
svm_rbf() %>%
set_engine("svm") %>%
set_mode("classification") %>%
translate() -> svm_mod
model1 <- caret::train(Class ~ ., data = wine_train,
method = "svm",
trControl = tune)
model1 <- caret::train(Class ~ ., data = wine_train,
method = "kernlab",
trControl = tune)
?caret::train
model1 <- caret::train(Class ~ ., data = wine_train,
method = "svmLinearWeights2",
trControl = tune)
model1 <- caret::train(Class ~ ., data = wine_train,
method = "svmLinearWeights2",
trControl = tune)
pacman::p_load(tidyverse, magrittr, # helper packages
tidymodels, # modeling packages
modeldata) # package including datasets
install.packages(c("broom", "car", "cli", "colorspace", "corrplot", "cpp11", "curl", "dplyr", "gargle", "ggplot2", "lme4", "Matrix", "mgcv", "mime", "rio", "slider", "xfun"))
install.packages(c("broom", "car", "cli", "colorspace", "corrplot", "cpp11", "curl", "dplyr", "gargle", "ggplot2", "lme4", "Matrix", "mgcv", "mime", "rio", "slider", "xfun"))
install.packages(c("broom", "car", "cli", "colorspace", "corrplot", "cpp11", "curl", "dplyr", "gargle", "ggplot2", "lme4", "Matrix", "mgcv", "mime", "rio", "slider", "xfun"))
install.packages(c("broom", "car", "cli", "colorspace", "corrplot", "cpp11", "curl", "dplyr", "gargle", "ggplot2", "lme4", "Matrix", "mgcv", "mime", "rio", "slider", "xfun"))
install.packages(c("broom", "car", "cli", "colorspace", "corrplot", "cpp11", "curl", "dplyr", "gargle", "ggplot2", "lme4", "Matrix", "mgcv", "mime", "rio", "slider", "xfun"))
install.packages("slider")
install.packages("slider")
